# In this file you can override any option defined in the reference files.
# Copy in parts of the reference files and modify as you please.

akka {

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 10
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://ClusterTestSystem@127.0.0.1:4711"
      "akka.tcp://ClusterTestSystem@127.0.0.1:4712"
    ]

    # how long to wait for one of the seed nodes to reply to initial join request
    seed-node-timeout = 5s

    # If a join request fails it will be retried after this period.
    # Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s

    metrics.enabled = off

    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    auto-down-unreachable-after = off

    # If this is set to "off", the leader will not move 'Joining' members to 'Up' during a network
    # split. This feature allows the leader to accept 'Joining' members to be 'WeaklyUp'
    # so they become part of the cluster even during a network split. The leader will
    # move `Joining` members to 'WeaklyUp' after 3 rounds of 'leader-actions-interval'
    # without convergence.
    # The leader will move 'WeaklyUp' members to 'Up' status once convergence has been reached.
    allow-weakly-up-members = on

    # The roles of this member. List of strings, e.g. roles = ["A", "B"].
    # The roles are part of the membership information and can be used by
    # routers or other services to distribute work to certain member types,
    # e.g. front-end and back-end nodes.
    # Roles are not allowed to start with "dc-" as that is reserved for the
    # special role assigned from the data-center a node belongs to (see the
    # multi-data-center section below)
    roles = ["Game","Table","Player","Network","Manager","Scheduler"]

    role {
      # Minimum required number of members of a certain role before the leader
      # changes member status of 'Joining' members to 'Up'. Typically used together
      # with 'Cluster.registerOnMemberUp' to defer some action, such as starting
      # actors, until the cluster has reached a certain size.
      # E.g. to require 2 nodes with role 'frontend' and 3 nodes with role 'backend':
      # <role-name>.min-nr-of-members = 1
      Game.min-nr-of-members = 1
      Table.min-nr-of-members = 1
      Player.min-nr-of-members = 1
      # Network.min-nr-of-members = 1
      # Manager.min-nr-of-members = 1
      # Scheduler.min-nr-of-members = 1
    }

    # Minimum required number of members before the leader changes member status
    # of 'Joining' members to 'Up'. Typically used together with
    # 'Cluster.registerOnMemberUp' to defer some action, such as starting actors,
    # until the cluster has reached a certain size.
    min-nr-of-members = 3

    # Enable/disable info level logging of cluster events
    log-info = on

    sharding {
      # The extension creates a top level actor with this name in top level system scope,
      # e.g. '/system/sharding'
      guardian-name = sharding

      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      # role = ""

      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # If the coordinator can't store state changes it will be stopped
      # and started again after this duration, with an exponential back-off
      # of up to 5 times this duration.
      coordinator-failure-backoff = 5 s

      # The ShardRegion retries registration and shard location requests to the
      # ShardCoordinator with this interval if it does not reply.
      retry-interval = 2 s

      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000

      # Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # Time given to a region to acknowledge it's hosting a shard.
      shard-start-timeout = 10 s

      # If the shard is remembering entities and can't store state changes
      # will be stopped and then started again after this duration. Any messages
      # sent to an affected entity may be lost in this process.
      shard-failure-backoff = 10 s

      # If the shard is remembering entities and an entity stops itself without
      # using passivate. The entity will be restarted after this duration or when
      # the next message for it is received, which ever occurs first.
      entity-restart-backoff = 10 s

      # Rebalance check is performed periodically with this interval.
      rebalance-interval = 10 s

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      # Only used when state-store-mode=persistence
      journal-plugin-id = ""

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      # Only used when state-store-mode=persistence
      snapshot-plugin-id = ""

      # Defines how the coordinator stores its state. Same is also used by the
      # shards for rememberEntities.
      # Valid values are "ddata" or "persistence".
      state-store-mode = "ddata"

      # The shard saves persistent snapshots after this number of persistent
      # events. Snapshots are used to reduce recovery times.
      # Only used when state-store-mode=persistence
      snapshot-after = 1000

      # The shard deletes persistent events (messages and snapshots) after doing snapshot
      # keeping this number of old persistent batches.
      # Batch is of size `snapshot-after`.
      # When set to 0 after snapshot is successfully done all messages with equal or lower sequence number will be deleted.
      # Default value of 2 leaves last maximum 2*`snapshot-after` messages and 3 snapshots (2 old ones + fresh snapshot)
      keep-nr-of-batches = 2

      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        rebalance-threshold = 10

        # The number of ongoing rebalancing processes is limited to this number.
        max-simultaneous-rebalance = 3
      }

      # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
      # Only used when state-store-mode=ddata
      waiting-for-state-timeout = 5 s

      # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
      # Only used when state-store-mode=ddata
      updating-state-timeout = 5 s

      # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
      # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
      # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
      # entity actors at a fix rate. The default strategy "all".
      entity-recovery-strategy = "all"

      # Default settings for the constant rate entity recovery strategy
      entity-recovery-constant-rate-strategy {
        # Sets the frequency at which a batch of entity actors is started.
        frequency = 100 ms
        # Sets the number of entity actors to be restart at a particular interval
        number-of-entities = 5
      }

      # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
      # The "role" of the singleton configuration is not used. The singleton role will
      # be the same as "akka.cluster.sharding.role".
      coordinator-singleton = ${akka.cluster.singleton}

      # Settings for the Distributed Data replicator.
      # Same layout as akka.cluster.distributed-data.
      # The "role" of the distributed-data configuration is not used. The distributed-data
      # role will be the same as "akka.cluster.sharding.role".
      # Note that there is one Replicator per role and it's not possible
      # to have different distributed-data settings for different sharding entity types.
      # Only used when state-store-mode=ddata
      distributed-data = ${akka.cluster.distributed-data}
      distributed-data {
        # minCap parameter to MajorityWrite and MajorityRead consistency level.
        majority-min-cap = 5
        durable.keys = ["shard-*"]

        # When using many entities with "remember entities" the Gossip message
        # can become to large if including to many in same message. Limit to
        # the same number as the number of ORSet per shard.
        max-delta-elements = 5

      }

      # The id of the dispatcher to use for ClusterSharding actors.
      # If not specified default dispatcher is used.
      # If specified you need to define the settings of the actual dispatcher.
      # This dispatcher for the entity actors is defined by the user provided
      # Props, i.e. this dispatcher is not used for the entity actors.
      use-dispatcher = ""
    }
  }
}